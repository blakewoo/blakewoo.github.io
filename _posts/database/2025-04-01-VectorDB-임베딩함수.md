---
title: Vector DB - 임베딩 함수(Embedding Functions)
author: blakewoo
date: 2025-4-15 16:00:00 +0900
categories: [Database]
tags: [Database, DBMS ,VectorDB]
render_with_liquid: false
---

# Vector DB - Embedding Functions
## 1. 개요
벡터 데이터 베이스에 넣으려면 데이터를 벡터화 해야한다.  
데이터를 벡터화하기 위해서는 벡터화 함수가 필요하다.   

벡터화 함수는 여러 종류가 있으며, 심지어 같은 종류의 데이터라도 종류가 많다.

## 2. 텍스트 벡터화
텍스트도 단어와 문자 두 가지로 나뉜다.

### 1) 단어 임베딩
이전에 포스팅 해둔 것이 있으니 [이곳](https://blakewoo.github.io/posts/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9B%8C%EB%93%9C%EC%9E%84%EB%B2%A0%EB%94%A9/) 을 참조하는게 낫다.

### 2) 문장/문서 임베딩
단순한 단어와는 다르게 문장 자체를 임베딩하는 방식이다.

- Sentence Transformers   
  Transformer 기반 모델을 변형하여 문장 임베딩에 특화된 모델이다.   
  ex) all-MiniLM-L6-v2, text-embedding-ada-002, all-mpnet-base-v2, paraphrase-MiniLM-L6-v2, AlBERT, BERT, RoBERTa
  
- Universal Sentence Encoder   
  구글 리서치 팀에서 개발했으며 
  문장 단위의 의미를 효과적으로 캡처하도록 설계되어 검색,
  분류 등 여러 자연어 처리 응용에서 활용된다.   
  이 역시 Transformer를 사용하는 모델도 있지만 DAN 모델을 사용하는 것도 있어서 별도로 분리했다.
  
## 3. 이미지 벡터화
여기서도 어느 기반이냐에 따라 종류가 나뉜다.

### 1) CNN 기반
전통적인 합성곱 신경망 (Convolutional Neural Network)을 이용한 방식으로 시각적 특징을 추출하여
중간 레이어의 출력 값을 벡터로 활용한다.   
ex) ResNet, VGG, EfficientNet

### 2) Transformer 기반
- Vision Transformer (ViT)   
  Google Research, Brain Team에서 나온 논문에서 비롯된 방식이다.
  이미지 패치를 토큰으로 변환한 후 Transformer를 적용하여 이미지의 전역 정보를 학습한다.


- CLIP (Contrastive Language–Image Pre-training)   
  이미지와 텍스트를 동일한 벡터 공간에 임베딩하는 모델이다.

## 4. 음성 벡터화
- Wav2Vec2   
  자가 지도 학습(self-supervised learning)을 통해 음성의 특징을 추출한다.
  보통 음성 인식 분야에서 많이 활용되며, 임베딩 벡터로 변환해 다양한 다운스트림 작업에 적용할 수 있다.


- OpenL3   
  오디오의 시간적, 주파수적 특징을 동시에 고려하여 임베딩을 생성한다.


- Whisper (OpenAI)   
  주로 STT(음성-텍스트 변환)에 쓰이지만, 내부적으로 학습된 표현(representation)을 임베딩으로 활용하는 경우도 있다.

## 5. Vector file format
### 1) fvecs
32‑bit float 형식의 벡터들을 순차적으로 기록하며 파일은 “벡터 개수” 정보 없이, 각 벡터 앞에 4바이트로 차원(dimension)을 쓰고
그 뒤에 해당 차원 수만큼의 float32 값을 잇따라 저장하는 구조이다. SIFT1M, GIST1M, Deep1M, SIFT1B 등 ANN‑Benchmarks용 대규모 벤치마크
데이터셋 배포에 사용한다

### 2) bvecs
8‑bit unsigned integer(바이트) 형식의 벡터들을 fvecs와 동일한 방식으로 저장하며
주로 바이너리(descriptor) 기반 피처(예: BRIEF, ORB 등)나 메모리 절약이 중요한 경우에 사용한다.

> ※ fvecs와 bvecs는 CRC와 같은 오류 검출 코드가 없으므로 스트리밍과 같은 상황에서 전달시 문제가 생길 수 있다.
{: .prompt-tip }

### 3) .npy / .npz
NumPy 배열 단일·다중 저장할때 사용하며 
실질 구조는 아래와 같다.
#### a. .npy
배열의 shape·dtype·엔디언 정보 포함한 바이너리 포맷

- Magic string과 버전 정보    
  첫 6바이트: 마법 문자열 \x93NUMPY    
  다음 1바이트: 메이저 버전 (예: \x01)   
  다음 1바이트: 마이너 버전 (예: \x00)   
  이 버전 번호는 NumPy 패키지 버전과 무관하며, 파일 포맷 자체의 버전을 나타낸다


- 헤더 길이 필드    
  버전 1.0: 다음 2바이트에 little‑endian uint16로 헤더 길이 (HEADER_LEN) 저장   
  버전 2.0 이상: 다음 4바이트에 little‑endian uint32로 확장된 헤더 길이 저장


- 헤더 데이터   
  HEADER_LEN 바이트만큼 크기이며 ASCII로 인코딩된 Python 리터럴 딕셔너리(pprint.pformat() 형식)을 취취한다.
  반드시 \n(newline)로 끝나며, 파일 포맷 버전 1.0은 전체(마법문자+4바이트+HEADER_LEN)가 16의 배수 되도록 스페이스(0x20)로 패딩된다.      
  딕셔너리 키:   
  - "descr": dtype.descr — numpy.dtype() 생성자 인자로 쓸 수 있는 설명자   
  - "fortran_order": bool — 배열이 Fortran(열-major) 순서인지 여부
  - "shape": tuple of int — 배열 차원 정보    


- 데이터 페이로드   
  헤더 뒤에는 배열 데이터가 바로 이어진다.   
  dtype.hasobject == True인 경우에는 Python pickle 형식으로 저장하며   
  그 외의 경우에는 메모리 상 C‑ 또는 Fortran‑contiguous 순서대로 원시 바이트 저장한다.   
  필요한 바이트 수는 shape의 전체 요소 개수 × dtype.itemsize로 계산 할 수 있다.

버전 별 차이점은 아래와 같다.

<table>
    <tr>
        <td>포맷 버전</td>
        <td>헤더 길이 필드 크기</td>
        <td>패딩 단위</td>
        <td>문자열 인코딩</td>
    </tr>
    <tr>
        <td>1.0</td>
        <td>uint16 (2B)</td>
        <td>16‑byte</td>
        <td>ASCII</td>
    </tr>
    <tr>
        <td>2.0</td>
        <td>uint32 (4B)</td>
        <td>16‑byte</td>
        <td>ASCII</td>
    </tr>
    <tr>
        <td>3.0</td>
        <td>uint32 (4B)</td>
        <td>64‑byte</td>
        <td>UTF‑8</td>
    </tr>
</table>

3.0 버전 부터는 UTF-8로 인코딩하기 때문에 한국어도 안전하게 포함할 수 있다.

#### b. .npz   
기본적으로는 내부에 여러 .npy 파일을 무압축 ZIP 아카이브로 저장하되
압축 설정 추가시 Deflate 압축된 ZIP으로 저장하며
완전한 배열 재구성 정보 보존, 메모리 매핑 지원이 장점이지만 NumPy에 종속되기 때문에 다른 곳에서 쓰기 어렵고
타 언어 호환성 제한되기 때문에 확장성이 떨어진다.


### 4) .tfrecord
TensorFlow 입력 파이프라인용 레코드 시퀀스를 저장하는 타입이다.
아래와 같은 구조로 이루어져있다.

<table>
    <tr>
        <td>타입</td>
        <td>크기(Bytes)</td>
        <td>내용</td>
    </tr>
    <tr>
        <td>uint64</td>
        <td>8</td>
        <td>length: 뒤따르는 데이터(payload)의 바이트 길이 (리틀 엔디언)</td>
    </tr>
    <tr>
        <td>uint32</td>
        <td>4</td>
        <td>length CRC32C: 위 length 필드의 CRC32C 체크섬을 “마스킹(masked)” 처리한 값</td>
    </tr>
    <tr>
        <td>bytes</td>
        <td>N</td>
        <td>data: 직렬화된 프로토버프 메시지(tf.train.Example 등)</td>
    </tr>
    <tr>
        <td>uint32</td>
        <td>4</td>
        <td>data CRC32C: 위 data 필드의 CRC32C 체크섬을 마스킹 처리한 값</td>
    </tr>
</table>

리틀 엔디언(Little‑Endian) 방식으로 정수 값을 저장한다.
마스킹(masked) CRC32C: 단순 CRC32C 대신, CRC 값을 비트 순환(rotate) 후 상수 0xA282EAD8를 더해 저장함으로써
“고유성(unique-ness)”과 오류 검출력을 높이는 처리를 한다.   
식으로 아래와 같다.

```c
masked_crc = ((crc >> 15) | (crc << 17)) + 0xa282ead8ul
```

기본적으로 Protocol Buffers(tf.train.Example) 직렬화 메시지의 연속 기록으로 이루어져으며
tf.data와 결합 시 파이프라인 최적화·병렬 I/O 지원을 지원하는 장점이 있으나 앞서 npy와 같이 특정 플랫폼
즉, TensorFlow 생태계 종속된다는 문제점이 있다.

> ※ 추가 업데이트 및 검증 예정이고, 아직 완벽하지 않으니 참고만 바란다.
{: .prompt-tip }


# 참고자료
- [밀버스 - 임베딩 함수](https://milvus.io/docs/ko/embeddings.md)
- [크로마 - 임베딩 함수](https://docs.trychroma.com/docs/embeddings/embedding-functions)
- [파인콘 - 임베딩 함수 고르기](https://www.pinecone.io/learn/series/rag/embedding-models-rundown/)
- [쿼드란트 - 임베딩 함수들](https://qdrant.tech/documentation/embeddings/)
- [밀버스 - 올바른 벡터 임베딩을 얻는 방법](https://milvus.io/ko/blog/how-to-get-the-right-vector-embeddings.md)
- [파인콘 - Creating Vector Embeddings](https://www.pinecone.io/learn/vector-embeddings/)
- Alexey Dosovitskiy, , Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." (2021).
- [OpenAI - CLIP: Connecting text and images](https://openai.com/index/clip/)
- Alec Radford, , Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. "Learning Transferable Visual Models From Natural Language Supervision." (2021).
- Alexei Baevski, , Henry Zhou, Abdelrahman Mohamed, and Michael Auli. "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations." (2020).
- Lan, Jiangyu, Shuai, Gao, Weiting, Zhang, Xindi, Hou, Minghui, Xi, Yuming, Zhang, Bo, Lei, Hongke, Zhang, and Xuemin Sherman, Shen. "OpenL3: Embedding Diverse Network Services Into MANETs Using Multi-Dimensional Identifier".IEEE Internet of Things Journal (2025): 1-1.
- Alec Radford, , Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. "Robust Speech Recognition via Large-Scale Weak Supervision." (2022).
- [Numpy - numpy.lib.format](https://numpy.org/devdocs/reference/generated/numpy.lib.format.html?utm_source=chatgpt.com)
- [텐서플로우 - 튜토리얼](https://www.tensorflow.org/tutorials/load_data/tfrecord?hl=ko)


