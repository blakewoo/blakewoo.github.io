---
title: 기계학습 - Deep learning - 순환신경망
author: blakewoo
date: 2025-2-18 15:30:00 +0900
categories: [Algorithm and AI]
tags: [Deep learning, AI, Machine Learning] 
render_with_liquid: false
use_math: true
---

# Deep learning
## 1. 순환신경망(Recurrent Neural Network, RNN)
### 1) 개요
앞서 입력층에서 출력층으로만 출력이 향했던 구조의 경우 피드포워드 신경망(Feed Forward Neural Network)라고 하는데, 이런 신경망의 경우
이전의 데이터에 대한 정보를 갖고 있지 않아 이전 데이터와 함께 순차적인 처리를 해야하는 문제의 경우 적절하지 않다.   
그렇다면 어떻게 구성해야 이전 입력데이터에 대한 정보를 갖고 있을 수 있을까?
은닉층 노드에서 활성화 함수를 통해 나온 결과를 출력층 방향으로도 내보내면서 다음 은닉층 노드의 입력으로도 사용하는 구조를
구성하여 신경망을 구성하면 된다. 이를 순환 신경망(Recurrent Neural Network)이라고 하며 시퀀스(sequence)를 요하는 일에 사용된다.   
그리고 은닉층에서 출력하면서 다음 시점의 자신에게 보내는 값을 은닉 상태(hidden state)라고 한다.

### 2) 구조
기본적으로 순환 신경망이란 다음과 같이 은닉층의 출력이 다시 은닉층 노드의 입력으로 들어가는 형태를 취한다.

![img.png](/assets/blog/algorithm/AI/deeplearning/순환신경망/img.png)

뉴런 단위로 그리면 위와 같은 그림의 형태로 나타나는데, 아래와 같이 시간 시점에 따라 표현하기도 한다.

![img_1.png](/assets/blog/algorithm/AI/deeplearning/순환신경망/img_1.png)

위와 같이 1개의 은닉층을 가질 수도 있지만, 2개를 가질 수도 있는데 이를 깊은 순환 신경망(Deep Recurrent Neural Network)라고 한다.

꼭 한방향으로만 값을 보내는 것이 아닌 양방향으로 보낼 수도 있다. 이전 시점의 입력뿐만 아니라 이후 시점의 입력 또한 예측에 필요할때
이런 구조를 사용하는데, 이를 양방향 순환 신경망(Bidirectional Recurrent Neural Network)라고 한다.

RNN은 입력과 출력의 길이를 다르게해서 다양한 용도로 사용할 수 있는데, 크게 나눠서 총 3가지로 나뉠 수 있다.

#### a. 다 대 다(many to many)

![img_4.png](/assets/blog/algorithm/AI/deeplearning/순환신경망/img_4.png)

다수의 입력에 대해 다수의 출력을 하는 다 대 다 구조의 모델은 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇이나
번역기, 개체명 인식이나 품사 태깅 같은 것을 할 수 있다.

#### b. 일 대 다(one to many)

![img_2.png](/assets/blog/algorithm/AI/deeplearning/순환신경망/img_2.png)

하나의 입력에 대해 여러개의 출력하는 일 대 다 구조의 모델은 하나의 이미지 입력에 대해서 사진의 제목을 출력하는
이미지 캡셔닝(Image Captioning) 작업에 사용할 수 있다.

#### c. 다 대 일(many to one)

![img_3.png](/assets/blog/algorithm/AI/deeplearning/순환신경망/img_3.png)

다수의 입력에 대해 하나를 출력하는 다 대 일 구조의 모델의 경우 입력받은 시퀀스가 어떤 것에 속하는지 속하지 않는지에 대해 사용할 수 있는데
가령 문서가 긍정인지 부정인지 판단하는 감성 분류(sentiment classification)이나 스팸 메일 분류(spam detection)에 쓸 수 있다.

### 3) 수식으로 표현

![img_5.png](/assets/blog/algorithm/AI/deeplearning/순환신경망/img_5.png)

RNN의 구조가 위와 같을 때 수식 표현은 아래와 같다.   
여기서 t는 현재 시점이고 $h_{t}$는 현재 시점의 은닉 상태이며 W는 가중치이다.

- 은닉층   
  $$  h_{t}=tanh(W_{x}x_{t}+W_{h}h_{t-1}+b) $$
  
- 출력층   
  $$ y_{t}=f(W_{y}h_{t}+b) $$   
  단, $f$는 비선형 활성화 함수 중 하나

$h_{t}$ 를 계산하기 위한 활성화 함수로는 주로 하이퍼볼릭탄젠트 함수(tanh)가 사용된다.
각각의 가중치 $W_{x},W_{h},W_{y}$의 값은 하나의 층에서 같은 값을 공유하나 은닉층이 2개일 경우 층마다 가중치는 서로 다르다.

출력층의 결과값인 $y_{t}$를 계산하기 위한 활성화함수는 푸는 문제에 따라 달라지며 이진 분류는 시그모이드 함수,
다중 클래스 문제는 소프트 맥스 함수를 사용할 수 있다.


# 참고 자료
- 인공지능 개념 및 응용 3판 Artificial Intelligence Concepts and Applications (2013년) - 도용태, 김일곤, 김종완, 박창현, 강병호 저,
  사이텍미디어
- [위키독스 - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)  
- Machine Learning, Tom Mitchell, McGraw Hill, 1997.
- [aws - RNN이란 무엇인가요?](https://aws.amazon.com/ko/what-is/recurrent-neural-network/)
