---
title: 기계학습 - Deep learning - 역전파와 에포크, 이터레이션
author: blakewoo
date: 2025-2-20 17:30:00 +0900
categories: [Algorithm and AI]
tags: [Deep learning, AI, Machine Learning] 
render_with_liquid: false
use_math: true
---

# Deep learning
## 1. 역전파
### 1) 개요
Deep learning에서 말하는 학습이란 적절한 가중치 w와 적절한 편향 b를 찾는 것이다.   
이를 찾기 위한 방법이 이전 포스팅에서 말했던 옵티마이저인데
역전파는 퍼셉트론으로 층층이 이루어진 계층을 역으로 타고 올라가면서 옵티마이저에 의한 최적화를 시행하는 것이라고 생각하면 된다.

#### ※ 기본적인 옵티아미저인 경사 하강법에 대한 설명
가장 쉬운 예시를 가지고 경사 하강법에 대한 설명을 좀 해보도록 하겠다.   
아래와 같은 그래프가 있다.

![img.png](/assets/blog/algorithm/AI/deeplearning/손실함수와옵티마이저/img.png)

이전 포스팅에서 봤던 그래프를 갖고 왔다.   
붉은 색 점이 실제값이고 선이 해당 점들에 대한 예측값이라고 할 때, 이 선이 각 점들을 가장 잘 반영하게끔 조정하려고 한다.
이 직선 그래프가 $y=wx+b$ 라고 할때 가중치 w와 편향 b를 수정해야한다.

b는 떼놓고 w만 가지고 생각할때 오차와 가중치에 대해 아래와 같은 그래프 형태가 나타난다.

![img.png](/assets/blog/algorithm/AI/deeplearning/역전파/img.png)

오차가 최저가되는 w 값외에는 오차가 오히려 증가하는 그래프를 보인다.   
그렇다면 최저가 되는 w는 어떻게 찾는가? 이는 위 그래프를 미분해본다면 알 수 있다.   
최저점 w의 우측이라면 미분시에 기울기가 0보다 크고, 좌측이라면 기울기가 0보다 작다.   
이를 이용하여 w에 어느정도의 값을 빼고 더할지 알 수 있다.   

따라서 일반화된 경사하강법을 시행하는 수식은 아래와 같다.

$$ w = w-\alpha \times \frac{d}{dw} \times J(w,b) $$
$$ b = w-b \times \frac{d}{dw} \times J(w,b) $$

위에서는 w에 대해서만 말했지만 일반적으로 b에 대해서 동일하게 시행한다.   
여기서 $\alpha$ 가 무엇이냐면 학습률을 말한다. 흔히들 하이퍼파라미터라고 하던데
이 값도 너무 과하면 진동하고, 너무 적으면 최적의 값을 찾는데까지 너무 오래 걸린다.

![img_1.png](/assets/blog/algorithm/AI/deeplearning/역전파/img_1.png)

### 2) 예시로 알아보기

![img.png](/assets/blog/algorithm/AI/deeplearning/역전파/img2.png)

위와 같은 인공신경망이 있다. 입력층, 은닉층, 출력층 총 3개의 층을 가지고
각 층의 뉴런은 2개씩에 모든 활성화 함수는 시그모이드 함수를 사용한다.

z값은 시그모이드 함수를 적용하지 않은 값이고 h값과 o값은 시그모이드 함수를 적용한 값이다.
여기서 편향 b를 고려하지 않는다고 할 때 $w_{5}$ 에 대하여 역전파를 계산해보겠다.

#### a. 순전파
$$z_{1} = w_{1}x_{1} + w_{2}x_{2} = 0.4 \times 0.2 + 0.1 \times 0.5 = 0.13 $$
$$z_{2} = w_{3}x_{1} + w_{4}x_{2} = 0.4 \times 0.2 + 0.3 \times 0.5 = 0.23 $$

$$h_{1} = sigmod( z_{1} ) = 0.53245431 $$
$$h_{2} = sigmod( z_{2} ) = 0.55724785 $$

$$z_{3} = w_{5}h_{1} + w_{6}h_{2} = 0.30028793 $$
$$z_{4} = w_{7}h_{1} + w_{8}h_{2} = 0.51202998 $$

$$o_{1} = sigmoid( z_{3} ) = 0.57451290 $$
$$o_{2} = sigmoid( z_{4} ) = 0.62534544 $$

손실 함수를 MSE로 할때

$$E_{o_{1}} = \frac{1}{2} (0.2 - 0.57451290)^{2} $$
$$E_{o_{2}} = \frac{1}{2} (0.4 - 0.62534544)^{2} $$

전체 오차는 

$$E_{total} = E_{o_{1}} + E_{o_{2}} = 0.09552023 $$

#### b. 역전파 1단계 ( $w_{5}$ 만 학습 예시)

학습 식은 아래와 같다.

$$ w_{k}^{+} = w_{k} - \alpha \times \frac{\partial E_{total}}{\partial w_{k}} $$

$w_{5}$ 를 학습한다고 할때 전체 오차에 대해 $w_{5}$에 대해서 미분해야한다.   
해당 편미분식은 아래와 같이 분해할 수 있다.

$$ \frac{\partial E_{total}}{\partial w_{5}} = \frac{\partial E_{total}}{\partial o_{1}} \times \frac{\partial o_{1}}{\partial z_{3}} \times \frac{\partial z_{3}}{\partial w_{5}} $$

각각의 식을 구하면

i)   
$$ \frac{\partial E_{total}}{\partial o_{1}} = -(0.2 - 0.574551290) = 0.3745129 $$

ii)   
$$ \frac{\partial o_{1}}{\partial z_{3}} = o_{1} \times (1-o_{1}) = 0.57451290 \times 0.4254891 = 0.24444783 $$

iii)   
$$ \frac{\partial z_{3}}{\partial w_{5}} = 0.53245431 $$

학습률( $\alpha$ )를 0.5 라고 할 때 $w_{5}$ 를 1번 학습하는 값은

$$ w_{5}^{+} = w_{5} - \alpha \times \frac{d}{dw} \times J(w,b) = 0.25 - 0.5 \times 0.04874559 = 0.225627205$$


## 2. 에포크와 이터레이션
### 1) 에포크(Epoch)
전체 데이터에 대해서 순전파와 역전파가 끝난 상태를 말한다.   
대상 데이터 셋에 대한 한번의 학습이 끝난 상태로 만약 50 에포크라고 한다면 대상 데이터 셋에 대해서 50번의 학습이 끝난 상태다. 

### 2) 배치크기(Batch size)
몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말한다.   
총 데이터가 1000개인데 100개씩 학습을 한다고 하면 배치 크기는 100이며 배치 수는 10이다.
이 배치 수는 아래에서 언급할 이터레이션이라고 한다.

### 3) 이터레이션(Iteration) 또는 스텝(Step)
한 번의 에포크를 끝내기 위해 필요한 배치의 수를 말한다. 배치 크기를 데이터 전체로 잡아 한번의 에포크를 했다면 이터레이션은 1이고
1000개를 100개씩 잡아 10번 했다면 이터레이션은 10이 되는 것이다.




# 참고 자료
- 인공지능 개념 및 응용 3판 Artificial Intelligence Concepts and Applications (2013년) - 도용태, 김일곤, 김종완, 박창현, 강병호 저,
  사이텍미디어
- [위키독스 - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)  
- Machine Learning, Tom Mitchell, McGraw Hill, 1997.
- [geeksforgeeks - Type of machine learning](https://www.geeksforgeeks.org/types-of-machine-learning/)
