---
title: 기계학습 - Deep learning - 워드임베딩
author: blakewoo
date: 2025-2-27 22:00:00 +0900
categories: [Algorithm and AI]
tags: [Deep learning, AI, Machine Learning] 
render_with_liquid: false
use_math: true
---

# Deep learning
# 워드임베딩(Word Embedding)
## 1. 개요
단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현(Dense Representation)으로 바꾸는 것을 말한다.   
이 밀집 표현에 대해서 알기 위해서는 희소 표현(Sparse Representation)에 대해 먼저 알아야한다.   

### 1) 희소표현(Sparse Representation)
벡터 또는 행렬의 대부분이 0을 표현되는 방법을 희소표현이라고 한다.   
예를 들어 원-핫 인코딩의 결과인 원-핫 벡터의 경우 아래와 같은 형태로 표현된다.

![img.png](/assets/blog/algorithm/AI/deeplearning/워드임베딩/img.png)

단어 리스트와 벡터 표현이 1대 1로 Mapping 되어 해당 단어를 나타내는 것이 아니라면
모두 0으로 처리한다. 이를 희소 표현이라고 하며 공간의 낭비가 매우 심한데
만약 100개의 단어를 표기하고 싶다면 길이 100의 벡터가 필요한 셈이다.

### 2) 밀집표현(Dense Representation)
지정된 차원의 크기로 표현하는 방법이다. 희소표현에서 밀집표현으로 바꾸는 여러 알고리즘을 거쳐서 나타낸다.   
처리된 희소표현은 0과 1이 아닌 지정된 차원의 크기만큼 실수로 표현된다.   
희소표현보다 적은 차원을 써서 밀집된 형태를 띄기에 밀집 표현이라고 부른다.

이러한 밀집 표현을 만드는 방법은 아래에 이어서 서술하겠다.

## 2. 종류
### 1) Word2Vec
### 2) FastText
### 3) Glove

> ※ 본 포스팅은 추가적으로 업데이트 될 예정이다. 내가 제대로 정리한게 맞는지 검증이 필요하다.
{: .prompt-tip }


# 참고 자료
- [위키독스 - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)  
- Tomas Mikolov et al, "Efficient Estimation of Word Representations in Vector Space", arxiv, 2013
- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation.
  In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,
  Doha, Qatar. Association for Computational Linguistics.
 
