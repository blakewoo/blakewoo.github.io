---
title: 기계학습 - Deep learning - 워드임베딩
author: blakewoo
date: 2025-2-27 22:00:00 +0900
categories: [Algorithm and AI]
tags: [Deep learning, AI, Machine Learning] 
render_with_liquid: false
use_math: true
---

# Deep learning
# 워드임베딩(Word Embedding)
## 1. 개요
단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현(Dense Representation)으로 바꾸는 것을 말한다.   
이 밀집 표현에 대해서 알기 위해서는 희소 표현(Sparse Representation)에 대해 먼저 알아야한다.   

### 1) 희소표현(Sparse Representation)
벡터 또는 행렬의 대부분이 0을 표현되는 방법을 희소표현이라고 한다.   
예를 들어 원-핫 인코딩의 결과인 원-핫 벡터의 경우 아래와 같은 형태로 표현된다.

![img.png](/assets/blog/algorithm/AI/deeplearning/워드임베딩/img.png)

단어 리스트와 벡터 표현이 1대 1로 Mapping 되어 해당 단어를 나타내는 것이 아니라면
모두 0으로 처리한다. 이를 희소 표현이라고 하며 공간의 낭비가 매우 심한데
만약 100개의 단어를 표기하고 싶다면 길이 100의 벡터가 필요한 셈이다.

### 2) 밀집표현(Dense Representation)
지정된 차원의 크기로 표현하는 방법이다. 희소표현에서 밀집표현으로 바꾸는 여러 알고리즘을 거쳐서 나타낸다.   
처리된 희소표현은 0과 1이 아닌 지정된 차원의 크기만큼 실수로 표현된다.   
희소표현보다 적은 차원을 써서 밀집된 형태를 띄기에 밀집 표현이라고 부른다.

이러한 밀집 표현을 만드는 방법은 아래에 이어서 서술하겠다.

## 2. 종류
### 1) Word2Vec
기본적으로 2개의 방식을 말하는데 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있다.   

#### a. CBOW(Continuous Bag of Words)
어떤 문장을 학습하는데 있어서 주변에 있는 단어를 입력으로 중간의 단어를 예측하는 방법이다.   
아래와 같은 예문이 있다고 가정해보자.   
["The", "fat", "man", "sat", "on", "the", "chair"]

index 0부터 끝까지 중심단어(Center word)로 선택하여 학습을 한다고 할때
주변의 몇 번째 단어까지 볼지 결정해야한다. 이 범위는 윈도우(Window)라고 한다.

윈도우 크기가 2이고, 중심단어가 "man"이라고 한다면 주변단어로 "the","man"과 "on","the"를
사용한다. 이름 원-핫 벡터로 나타낸다면 아래와 같다.

중심단어 : [0,0,1,0,0,0,0]   
주변단어 : [1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,0,1,0,0,0],[0,0,0,0,1,0,0]

이를 처음부터 끝 단어까지 모두 중심단어로 선정하여 일종의 데이터 셋을 만든다.   
이 방법을 슬라이딩 윈도우라고한다.   
이렇게 슬라이딩 윈도우를 만들었다면 아래 그림을 보자

![img_1.png](/assets/blog/algorithm/AI/deeplearning/워드임베딩/img_1.png)

CBOW는 위와 같은 구조로 되어있다. 위 그림은 Word2Vec가 처음 발표된
"Efficient Estimation of Word Representations in Vector Space" 논문에서 발췌한 것이다.

먼저 Input에 원-핫 벡터로 Mapping한 값들을 각각 넣고, 투사층(Projection Layer)라고 불리는
은닉층을 통과시켜 출력층을 뽑아낸다.
여기서 Projection Layer는 활성화 함수가 존재하지 않는 은닉층으로 생각하면 편하며
결과 값으로는 중심 단어의 원-핫 벡터와 비교하여 역전파로 학습을 하면 된다.

입력값에서 나오는 값과 곱해지는 W값과 투사층에서 나오는 값과 곱해지는 W'값을 학습하는게 목표이다.
총 단어집합의 크기 V와, 지정한 차원 M이라면 W값은 V x M이고 W'은 M x V 형태의 행렬이 된다.

W와 W'을 이용하여 각 값을 곱해서 나온 결과 값은 사실상 W행렬의 i번째 행을 그대로 lookup해오는 것과 같기에
W행렬의 곱을 lookup table이라고 부른다.   

실질적으로 학습시킨 결과는 윈도우 크기의 중간이므로 곱한 값들에 대해 평균을 구해줘야 실질적으로 구하려는
값을 구할 수 있다.
따라서 슬라이드 안의 값들을 W로 곱한 것에서 평균 값을 구하고, 이후에 W' $\times$ v 를 해준뒤
크로스 엔트로피 함수를 이용하여 다시 원-핫 벡터에 매핑시키면 원래 값으로 나오게 할 수 있다.

#### b. Skip-Gram
CBOW에서 주변 단어를 통해 중심 단어를 예측했다면, Skip-gram은 중심 단어에서 주변 단어를 예측한다.
이전과 같이 윈도우 크기가 2이고, 중심단어가 "man"이라고 한다면 주변단어로 "the","man"과 "on","the"를
사용한다. 이름 원-핫 벡터로 나타낸다면 아래와 같다.

중심단어 : [0,0,1,0,0,0,0]   
주변단어 : [1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,0,1,0,0,0],[0,0,0,0,1,0,0]

이를 인공 신경망으로 도식화해보면 아래와 같다.

![img.png](/assets/blog/algorithm/AI/deeplearning/워드임베딩/img_2.png)

아까와는 다르게 input이 1개고 투사층을 지나서 4개로 나뉜 것을 볼 수 있다.
절차에 대해서는 크게 다르지 않다.
단, 평균 값은 내지 않으며 각 W' 값을 곱해서 나온 결과를 크로스 엔트로피 함수를 취해서 각 데이터의 원-핫 벡터로 변환할 수 있다.

기본적으로 Skip-gram이 CBOW보다 성능이 좋으나 윈도우의 크기에 따라 임베드 벡터로 변환하는 시간이 좀 더 오래 걸린다고 한다.

### 2) FastText
### 3) Glove

> ※ 본 포스팅은 추가적으로 업데이트 될 예정이다. 내가 제대로 정리한게 맞는지 검증이 필요하다.
{: .prompt-tip }


# 참고 자료
- [위키독스 - 딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155)  
- Tomas Mikolov et al, "Efficient Estimation of Word Representations in Vector Space", arxiv, 2013
- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation.
  In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,
  Doha, Qatar. Association for Computational Linguistics.
- Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. ‘Enriching Word Vectors with Subword Information’.
  arXiv [Cs.CL], 2017. arXiv. http://arxiv.org/abs/1607.04606.  
