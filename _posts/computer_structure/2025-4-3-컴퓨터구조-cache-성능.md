---
title: 컴퓨터 구조 - CPU 구조 - Cache 성능
author: blakewoo
date: 2025-4-6 16:00:00 +0900
categories: [Computer structure]
tags: [Computer structure, CPU, Cache, Memory] 
render_with_liquid: false
use_math: true
---

# Cache 성능
## 1. 개요
기본적으로 Memory에 대한 성능을 이야기 할때 AMAT(Average Memory Access Time)를 가지고 말한다.
이 AMAT는 아래와 같이 산출한다.

$$ AMAT = Hit Time + Miss Rate + Miss Penalty $$

- Hit Time : Cache에 data가 있고 이 데이터를 갖고 오기까지의 시간을 말한다.
- Miss Rate : Cache에 접근시 어느정도 비율로 Cache miss가 나는지를 말한다. 
- Miss Penalty : Cache miss가 발생시 하위 메모리 층에서 데이터를 갖고 오는 시간을 말한다.

따라서 성능이 높다는 것은 AMAT이 작다는 뜻이고, 이 AMAT을 줄이기 위해서는 위 세 요소를 줄여야하는 것이다.

## 2. Cache miss
Hit Time 이전에 Cache miss에 대해서 알아보도록 하겠다.    
일반적으로 우리가 Cache miss라 부르는 것은 사실 총 3가지 경우가 있다.

### 1) Compulsory or cold misses
프로그램 시작시 모든 Cache가 비어있기 때문에 어쩔수없이 발생하는 Cache miss이다.   
피할수 없기 때문에 Compulsory 혹은 Cold(예열되지 않았음) miss로 불린다.   
이런 경우는 미리 Cache에 데이터를 넣어두면 피할 수 있다. 이를 pre-fetch라고 한다.

### 2) Conflict or interference misses
대상 Index에 목표하는 데이터가 아닌 다른 데이터가 점유하고 있어서 Conflict로 인해 발생하는 Cache miss이다.    
hashing collision 문제와 비슷하다고 생각하면 된다.   
이는 Cache를 associativity 하게 구성하면 conflict 가 덜 발생하게 된다.

### 3) Capacity misses
Cache 용량이 크지 않아서 생기는 문제이다.   
어차피 Cold misses 제외하고는 Cache 크기가 작아서 생기는 문제가 아닌가? 할 수 있겠지만 좀 다르다.   
정확하게는 Cache가 Working set, 즉 프로그램을 구동할때 필요한 최소한의 명령어와 데이터를 다 담지 못하기 때문에
발생하는 문제이다.   
이는 Cache 용량을 늘리거나, 데이터의 양을 줄이면 해결된다.

### ※ Capacity misses와 Conflict misses를 구분하는 법
그럼에도 불구하고 현재 발생중인 Cache miss가 Capacity misses인지 Conflict misses인지 헷갈린다면,
현재 구동중인 Cache 구조가 Fully associative cache라고 생각해봤을때도 해당 문제가 발생한다면
이는 Capacity misses인 것이다.

## 3. Cache block
### 1) Cache block 크기와 Address의 상관 관계
여기서 말하는 Block이란 Cache의 실질적인 Data가 담겨있는 곳의 크기를 말한다.   
그리고 Cache miss시에 메모리에서 한번에 읽어오는 크기를 말하기도 한다. 
Cache block에 대한 크기는 Address와 Tag bit와 Index bit, 그리고 전체 bit만 주면 유추해볼 수 있다.    
예를 들어 가령 아래와 같은 구조라고 해보자.   

- 총 Address bit : 32
- Tag bit : 20
- Index bit : 8
- byte offset bit : 4

Direct Mapped Cache라고 할 때, block의 크기는 어떻게 되며 총 Cache의 크기는 어떻게 될까?   
먼저 block의 크기는 간단한데 bit offset의 크기를 승수로 계산하면 된다.

$$ 2^{4} = 16 $$

총 16bytes의 크기이며, 1 word를 4bytes라고 한다면 4 word만큼 들어갈 수 있다.    
캐시 block 크기를 산정했으니 총 Cache Data의 크기도 산정해볼 수 있다.   
index만큼 cache 블럭 수가 있기 때문이다.   

$$ 2^{8} = 256 $$

총 256개의 블럭이 있고 블럭마다 사이즈가 16bytes이니

$$ 256 \times 16 = 4096 = 4 \times 1024 = 4KB $$

Cache의 총 Data block는 4KB가 되는 것이다.

### 2) Cache block과 성능
이 Cache block의 크기에 따라 성능이 달라진다.   
만약 Cache block이 커진다고 가정해보자. 그러면 전체 Address는 동일하고, 전체 Cache의 사이즈가 이전과 동일하다면
전체 Cache Block 수가 줄어든다. Block이 커졌기에 한번에 갖고 오는 연속 데이터가 커지니 Spatial locality 가 높은
프로그램의 경우 성능이 올라가나, Block 개수가 줄었기 때문에 Temporal Locality가 높은 프로그램의 경우
성능이 저하되며 Memory bandwidth를 많이 요한다.

그렇다면 반대로 생각해서 block 사이즈가 작아진다면 어떻게 될까? 동일하게 Address, 전체 사이즈가 동일하다면
전체 Cache block 수가 많아지나, Block이 작아졌기에 Spatial Locality 가 높은 프로그램이라면 성능이 떨어지지만
Cache block의 개수가 많아졌기 때문에 Temporal Locality가 높은 프로그램이라면 오히려 성능이 올라갈 수 있다.

이를 그림으로 그려보면 아래와 같다.

![img.png](/assets/blog/cs/cpu_structure/cache/performance/img.png)

Block 사이즈가 커지면 Miss Rate가 내려가다가 일정 지점을 이후로 쭉 올라가는 걸 볼 수 있는데
이 지점이 가장 최적의 사이즈이고, 최근 일반적으로는 이 사이즈가 64Bytes로 잡혀있다.

## 4. Hit time 줄이기
### 1) 상위 Memory는 사이즈를 작고 간단하게 만든다.
기본적으로 상위 Memory는 아주 빨라야한다. L1 Cache의 경우 1 ~ 3 cycle 안에 데이터를 가져와야한다.
따라서 작고, 간단하게 만들어져야한다.

기본적으로 작다는 것은 칩 안에 있을 수 있다는 것이고 칩 밖으로 나가면 거리가 늘어나기 때문에 시간이 느려진다.   
L1 캐시의 경우 Direct Mapped Cache로 만들어지는데, associative cache가 아닌 이유는
associative 하게 만들려면 별도의 회로가 필요하고 이 회로가 성능을 떨어뜨리는 원인이 되기 때문이다.

아래의 그림을 보자

![img.png](/assets/blog/cs/cpu_structure/cache/performance/img_1.png)   
출처 :  [Computer Architecture, Dr Ranjani Parthasarathi](https://www.cs.umd.edu/~meesh/411/CA-online/chapter/cache-optimizations-iii/index.html)

위의 그림을 보면 간단하고 작은 사이즈일 수록 속도가 빨라짐을 알 수 있다.

### 2) Psuedo-Associative Cache
위에서 간단한 게 빠르다곤 했지만 사실 n-way associative cache로 만들면 cache conflict가 n분의 1만큼 줄어드므로
이를 포기하기엔 아쉽다.   
따라서 이 n-way associative cache 와 유사하게 만들되 성능 감소가 적은 방법이 있을지 누가 생각했다.   
아래의 그림을 보자

![img_1.png](/assets/blog/cs/cpu_structure/cache/performance/img_2.png)   
출처 : [ScienceDirect - Set-Associative Cache](https://www.sciencedirect.com/topics/computer-science/set-associative-cache)

위 그림은 2-way associative cache이다.   
잘 보면 mux를 통해 Data가 나가는 것을 볼 수 있다. MUX 회로는 way가 많아질 수록 느려지고 복잡해진다.   
이는 다수의 게이트를 거쳐야하기 때문에 느려지는 것이다.  
따라서 이 mux를 없애버릴 방법만 찾으면 되었다. 그렇게 해서 개발 된 것이 Prediction하는 방식이다.
이는 흡사 branch prediction 같이 prediction하는 것으로 MRU 로직을 부착하여 해당 Cache에 대해서
Prediction을 한 후에 hit인지 아닌지를 이후에 판단하여 hit면 바로 사용할 수 있게 변경함으로써 성능을 높일 수 있게 되었다.

## 5. 캐시 Bandwidth 늘리기
### 1) 캐시 파이프라인
캐싱에 관한 부분도 파이프라인을 하면 성능이 올라간다.   
Throughput 이 증가하니 당연히 성능이 올라간다.   
하지만 이 경우 분기 예측에 실패하면 패널티가 더 커진다.

### 2) Multi-ported/Multi-banked 캐시
한번에 많은 양을 읽어올 수 있거나 많이 쓸수 있다면 성능이 올라간다.   
왜냐면 처리량이 올라가기 때문이다.   
이 방식을 구현하기 위해서는 두 가지로 나눌 수 있다.

#### a. Multi-ported cache
진짜 한 개의 캐시에 다수 접근이 가능하게끔 만든 캐시이다.   
이 경우 1개 이상의 접근이 가능하나 두 가지 문제점이 생긴다.
- 캐시 영역이 커진다 (회로가 복잡하기 때문에 영역을 많이 차지한다)
- hit time이 늘어난다(회로가 복잡하기 때문에 delay가 생긴다)

#### b. Multi-banked cache
한번에 한 개만 접속할 수 있는 캐시를 병렬적으로 연결한 형태의 캐시이다.   
이렇게 만들면 회로가 복잡하지도 않고 병렬성도 챙길수 있어서 이득이지만
접근 데이터가 같은 캐시에 들어있다면 병렬성을 살릴 수 없다(bank conflict)
따라서 데이터 배치 방식이 중요한 방법이다.

### 3) Non-blocking or Lockup Free Cache
일반적으로 Cache miss가 나면 stall을 통해서 data를 갖고올때까지 기다리지만(blocking)
Non-blocking cache(혹은, Lockup Free Cache) 구조의 캐시는 일단 어디에 기재해두고 계속해서 진행하는 형태이다.

miss를 처리하는 동안 hit를 처리할 수 있으며(hit-under-miss), miss간에도 miss를 허용하는 것(miss-under-miss)   
이 두 가지 아이디어가 기반이 되어 만들어진 캐시로, 이 miss를 얼마나 용인하고 기재할 것인가는 Cache의 정책과 하드웨어에 달려있다.

이를 운용하기 위해선 특별한 하드웨어가 필요한데 이를 Miss Status Handling Registers(MSHR)이라고 부른다.   
어느부분이 cache에 데이터를 갖고오기를 기다리고 있다는 일종의 메모장이다.   
아래와 같은 구조를 가진다. (세부 bit는 설계에 따라 달라질 수 있다.)

![img.png](/assets/blog/cs/cpu_structure/cache/performance/img_3.png)

왼쪽을 가칭으로 address block, 오른쪽을 offset block이라고 해보자.
address block은 다수의 offset 블록과 같이 엮여있다. 
위와 같은 구조에서 아래와 같은 명령어 셋이 있다고 해보자.

```
lw $1, 100($2)
sw $3, 104($2)
```

만약에 100($2) 주소의 Block Address가 502(Decimal), Block Offset이 4(Decimal)이고 Cache miss가 났다면?

![img_1.png](/assets/blog/cs/cpu_structure/cache/performance/img_4.png)

위와 같이 block address가 기재되고, offset Block에 세부 내용이 기재된다.
address block의 Valid는 유효한 값인지, Block Address는 Address가 어떻게 되는지, Issued는 해당 내용을 메모리에 요청했는지이다.
offset Block의 경우, Valid는 유효한 값인지, Type은 어떤 연산인지, Block offset은 offset이 어떤 값인지, 갖고 온 값이 어느 레지스터로
가야하는지이다.

그 다음 sw의 명령어의 경우 104($2) 주소가 Block Address가 502에 offset이 5라고 한다면 아래와 같이 된다.

![img_2.png](/assets/blog/cs/cpu_structure/cache/performance/img_5.png)

요청한 값이 cache에 왔다면 offset Block에 Valid를 0으로 처리해주고, 모든 offset Block에서 처리가되었다면 Address block의 Valid 역시
0으로 처리해준다.

만약 처리과정에서 offset Block의 슬럿이 꽉 찼다면 어떻게 될까?   
그땐 빈 슬럿이 날때까지 stall로 기다리게 된다.


> ※ 추가 업데이트 및 검증 예정이고, 올라간 부분도 아직 완벽하지 않으니 참고만 바란다.
{: .prompt-tip }


# 참고자료
- Computer Organization and Design-The hardware/software interface, 5th edition, Patterson, Hennessy, Elsevier
- 서강대학교 이혁준 교수님 강의자료 - 고급 컴퓨터 구조
- [Computer Architecture, Subtitle:Engineering And Technology ,Author:Dr Ranjani Parthasarathi](https://www.cs.umd.edu/~meesh/411/CA-online/chapter/cache-optimizations-iii/index.html)
- [ScienceDirect - Set-Associative Cache](https://www.sciencedirect.com/topics/computer-science/set-associative-cache)
